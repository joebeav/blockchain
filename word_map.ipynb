{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest words map\n",
    "Find and plot the k-nearest words for top tf-idf words in reddit comments corpus from Jan. 2017.\n",
    "### 1. Reorganize the corpus into a collection documents - one thread per document\n",
    "First, let's print out one comment and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"parent_id\":\"t1_dbufkak\",\"id\":\"dbumnvn\",\"edited\":false,\"created_utc\":1483228806,\"distinguished\":null,\"author_flair_css_class\":null,\"author_flair_text\":null,\"controversiality\":0,\"subreddit_id\":\"t5_2wlj3\",\"retrieved_on\":1485679713,\"link_id\":\"t3_5l72cl\",\"author\":\"twigwam\",\"score\":1,\"gilded\":0,\"stickied\":false,\"body\":\"OMGosh im an idiot this week.  Thank you\",\"subreddit\":\"CryptoCurrency\"}\\n'\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "import json\n",
    "import time\n",
    "f_in = bz2.BZ2File(\"data/2017-01.bz2\").readlines() \n",
    "# each bytes string is json like in this dataset\n",
    "now = time.time()\n",
    "print (f_in[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's aggregates comments based on parent_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization took 10.578137159347534s.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w*[A-z]+\\w*')\n",
    "comments = [(json.loads(line)['link_id'], tokenizer.tokenize(json.loads(line)['body'].lower())) for line in f_in]\n",
    "print (\"Tokenization took {}s.\".format(time.time() - now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentation took 352.1107487678528s.\n"
     ]
    }
   ],
   "source": [
    "from collections import *\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "now = time.time()\n",
    "documents = defaultdict(list)\n",
    "tags_of_interest = ['NN', 'NNS', 'NN$', 'VBD', 'VBP', 'VBX', 'VBG', 'VB']\n",
    "noun_verb = []\n",
    "eng_stopwords = stopwords.words('english')\n",
    "for (k,v) in comments:\n",
    "    noun_verb += [i[0] for i in nltk.pos_tag(v) if i[1] in tags_of_interest]\n",
    "    filtered_words = [word for word in v if word not in eng_stopwords]\n",
    "    documents[k] += filtered_words\n",
    "print (\"Documentation took {}s.\".format(time.time() - now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156850 9690\n"
     ]
    }
   ],
   "source": [
    "print(len(comments), len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = time.time()\n",
    "tf_df = defaultdict(Counter)\n",
    "for item in documents.items():\n",
    "    document = item[1]\n",
    "    for word in set(document):\n",
    "        tf_df[word].update(tf = document.count(word), df = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating TFIDF took 81.41364312171936s.\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "tfidf = {}\n",
    "N = len(documents)\n",
    "for (k, v) in tf_df.items():\n",
    "    tfidf[k] = v['tf']*log(N/v['df'])\n",
    "sorted_list = sorted(tfidf.items(), key = lambda t:-t[1])\n",
    "print (\"Calculating TFIDF took {}s.\".format(time.time() - now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting took 182.1247899532318s.\n"
     ]
    }
   ],
   "source": [
    "now = time.time()\n",
    "noun_verb_unique = list(set(noun_verb))\n",
    "sorted_clean = [word[0] for word in sorted_list if len(word[0]) > 2 and word[0] in noun_verb_unique]\n",
    "top100 = sorted_clean[:100]\n",
    "print (\"Sorting took {}s.\".format(time.time() - now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GloVe representations for top100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove = open(\"data/glove.840B.300d.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51260"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "now = time.time()\n",
    "w2v = defaultdict(list)\n",
    "for line in glove:\n",
    "    word = line.split(\" \")[0]\n",
    "    if not word in sorted_clean:\n",
    "        continue\n",
    "    array = [float(i) for i in line.split(' ')[1:]]\n",
    "    w2v[word] = array\n",
    "\n",
    "with open(\"intermed/w2v.json\", 'w') as f:\n",
    "    json.dump(w2v, f)\n",
    "f.close()\n",
    "\n",
    "print (\"Word to vec mapping took {}s.\".format(time.time() - now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate cosine similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like not every word here is included in the GloVe pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = time.time()\n",
    "for word in top100:\n",
    "    if len(w2v[word]) == 0:\n",
    "        top100.remove(word)\n",
    "print (len(top100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(vec1, vec2):\n",
    "    return abs(np.dot(np.transpose(vec1), vec2) / (np.dot(np.transpose(vec1), vec1) + np.dot(np.transpose(vec2), vec2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "cos_sim = defaultdict(float)\n",
    "word_pairs = list(itertools.combinations(top100, 2))\n",
    "for pair in word_pairs:\n",
    "    cos_sim[pair] = cosine_sim(w2v[pair[0]], w2v[pair[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_sims = sorted(cos_sim.items(), key = lambda t:(t[0][0], -t[1]))\n",
    "print (\"Calculating cosine similarities took {}s.\".format(time.time() - now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sorted_sims[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Construct a list for top-k (k=5) nearest words for each words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-order the keywords in a word pair into alphabetic. Keep only the top k pairs with highest similarities. But this doesn't necessarily find the top-k for all the words. This is wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = time.time()\n",
    "ordered_keys = []\n",
    "for pair in sorted_sims:\n",
    "    ordered_key = sorted(pair[0])\n",
    "    ordered_keys.append([ordered_key, pair[1]])\n",
    "print (\"Sorting took {}s.\".format(time.time() - now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, sort the similarities list by the first key word (item\\[0\\]\\[0\\]) and the similarity values. \n",
    "Take the top k item\\[0\\]\\[1\\] for each item\\[0\\]\\[0\\] and construct a new list. \n",
    "Sort the new list by the second key word (item\\[0\\]\\[1\\]) and the similarity values. \n",
    "If one item\\[0\\]\\[1\\] has more than k pairs, keep the top k pairs with highest similarities and dump the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 5 \n",
    "N = len(top100)\n",
    "top_k = []\n",
    "current_word = \"\"\n",
    "past_word = []\n",
    "for i, pair in enumerate(sorted_sims):\n",
    "    if current_word != pair[0][0]:\n",
    "        current_word = pair[0][0]\n",
    "        top_k += sorted_sims[i : i + 5]\n",
    "\n",
    "top_k = sorted(top_k, key = lambda t:(t[0][1], -t[1]))\n",
    "top_k_2 = []\n",
    "counter = 0\n",
    "current_word = \"\"\n",
    "for pair in top_k:\n",
    "    if current_word == pair[0][1]:\n",
    "        counter = counter + 1\n",
    "    else:\n",
    "        counter = 0\n",
    "        current_word = pair[0][1]\n",
    "    if counter < k:\n",
    "            top_k_2.append(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now top_k_2 is the list that we are looking for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(len(top_k), len(top_k_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this data so that we don't have to re-run the whole model when the program crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"intermed/top_k_pairs.json\", 'w') as f:\n",
    "    json.dump(top_k_2, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extra. Convert Word2vec from .bin to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import gensim\n",
    "\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# model.save_word2vec_format('data/GoogleNews-vectors-negative300.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. plot the network of similar words\n",
    "Load the top k pairs from the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"top_k_pairs.json\", 'r') as f:\n",
    "    top_k_pairs = json.load(f)\n",
    "f.close()\n",
    "len(top_k_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the resolution environment for better quality plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "G = nx.Graph()\n",
    "edges = [(pair[0][0], pair[0][1]) for pair in top_k_pairs[:100]]\n",
    "edgewidth = [pair[1] * 20 for pair in top_k_pairs[:100]]\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    pos = nx.nx_agraph.graphviz_layout(G)\n",
    "except:\n",
    "    pos = nx.spring_layout(G, iterations=20)\n",
    "    \n",
    "plt.figure(figsize=(16, 8))\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.3, width=edgewidth, edge_color='g')\n",
    "nx.draw_networkx_nodes(G, pos, node_size=5, node_color='w', alpha=0.6)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.4, node_size=0, width=1, edge_color='k')\n",
    "nx.draw_networkx_labels(G, pos, fontsize=16)\n",
    "font = {'fontname': 'Helvetica',\n",
    "        'color': 'k',\n",
    "        'fontweight': 'bold',\n",
    "        'fontsize': 16}\n",
    "plt.axis('off')\n",
    "plt.savefig(\"results/top_k_pairs.png\", dpi = 600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}